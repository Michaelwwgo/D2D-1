{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54986f",
   "metadata": {},
   "source": [
    "### 数据生成的一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feasible_Loc_Init和loc_init：用来生成DUE和CUE的位置\n",
    "'''\n",
    "def Feasible_Loc_Init(Cur_loc, Size_area, Dist_TX_RX):\n",
    "    temp_dist = Dist_TX_RX * (np.random.rand(1, 2) - 0.5)\n",
    "    temp_chan = Cur_loc + temp_dist\n",
    "    while (np.max(abs(temp_chan)) > Size_area / 2) | (np.linalg.norm(temp_dist) > Dist_TX_RX):\n",
    "        temp_dist = Dist_TX_RX * (np.random.rand(1, 2) - 0.5)\n",
    "        temp_chan = Cur_loc + temp_dist\n",
    "    return temp_chan\n",
    "\n",
    "\n",
    "def loc_init(Size_area, Dist_TX_RX, Num_D2D, Num_Ch):\n",
    "    tx_loc = Size_area * (np.random.rand(Num_D2D, 2) - 0.5)\n",
    "    rx_loc = np.zeros((Num_D2D + 1, 2))\n",
    "    for i in range(Num_D2D):\n",
    "        temp_chan = Feasible_Loc_Init(tx_loc[i, :], Size_area, Dist_TX_RX)\n",
    "        rx_loc[i, :] = temp_chan\n",
    "    tx_loc_CUE = Size_area * (np.random.rand(Num_Ch, 2) - 0.5)\n",
    "\n",
    "    return rx_loc, tx_loc, tx_loc_CUE\n",
    "\n",
    "\n",
    "'''\n",
    "Feasible_Loc_Update和loc_update：暂时不用，后续可以用来更新DUE和CUE的位置\n",
    "'''\n",
    "def Feasible_Loc_Update(Cur_RX_loc, Cur_TX_loc, Size_area, Dist_TX_RX, Delta_mov):\n",
    "    temp_chan = 0\n",
    "    temp_dist = 2 * Dist_TX_RX\n",
    "\n",
    "    while (np.max(abs(temp_chan)) > Size_area / 2) | (np.linalg.norm(temp_dist) > Dist_TX_RX):\n",
    "        temp_dir = np.random.rand()\n",
    "        temp_dist_delta = [Delta_mov * np.cos(2 * np.pi * temp_dir), Delta_mov * np.sin(2 * np.pi * temp_dir)]\n",
    "        temp_chan = Cur_RX_loc + temp_dist_delta\n",
    "        temp_dist = Cur_TX_loc - temp_chan\n",
    "\n",
    "    return temp_chan\n",
    "\n",
    "\n",
    "def loc_update(Size_area, Dist_TX_RX, rx_loc, tx_loc, tx_loc_CUE, Delta_mov):\n",
    "    tx_loc_update = tx_loc\n",
    "    rx_loc_update = rx_loc\n",
    "    tx_loc_CUE_update = tx_loc_CUE\n",
    "\n",
    "    Num_D2D = np.shape(tx_loc)[0]\n",
    "    Num_CH = np.shape(tx_loc_CUE)[0]\n",
    "\n",
    "    ## Determine the location of D2D users\n",
    "    for i in range(Num_D2D):\n",
    "        ## Use 2*size_area to deactivate the second condition\n",
    "        tx_loc_update[i, :] = Feasible_Loc_Update(tx_loc[i, :], tx_loc_update[i, :], Size_area, 2 * Size_area,\n",
    "                                                  Delta_mov)\n",
    "        rx_loc_update[i, :] = Feasible_Loc_Update(rx_loc[i, :], tx_loc_update[i, :], Size_area, Dist_TX_RX, Delta_mov)\n",
    "\n",
    "    ## Determine the location of CUE Users\n",
    "    for i in range(Num_CH):\n",
    "        tx_loc_CUE_update[i, :] = Feasible_Loc_Update(tx_loc_CUE[i, :], tx_loc_CUE[i, :], Size_area, 2 * Size_area,\n",
    "                                                      Delta_mov)\n",
    "\n",
    "    return rx_loc_update, tx_loc_update, tx_loc_CUE_update\n",
    "\n",
    "\n",
    "'''\n",
    "ch_gen：生成整个信道环境的函数\n",
    "'''\n",
    "def ch_gen(Size_area, D2D_dist, Num_D2D, Num_Ch, Num_samples, Delta_mov=0.0833, PL_alpha=38., PL_const=34.5):\n",
    "    ch_w_fading = []\n",
    "\n",
    "    ## Perform initialization just once and the rest channel is generated by moving users\n",
    "    rx_loc, tx_loc, tx_loc_CUE = loc_init(Size_area, D2D_dist, Num_D2D, Num_Ch)\n",
    "\n",
    "    for i in range(Num_samples):\n",
    "        \n",
    "        #rx_loc, tx_loc, tx_loc_CUE = loc_update(Size_area, D2D_dist, rx_loc, tx_loc, tx_loc_CUE, Delta_mov)\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            rx_loc, tx_loc, tx_loc_CUE = loc_init(Size_area, D2D_dist, Num_D2D, Num_Ch)\n",
    "\n",
    "        ch_w_temp_band = []\n",
    "        for j in range(Num_Ch):\n",
    "            tx_loc_with_CUE = np.vstack((tx_loc, tx_loc_CUE[j]))\n",
    "            ## generate distance_vector\n",
    "            dist_vec = rx_loc.reshape(Num_D2D + 1, 1, 2) - tx_loc_with_CUE\n",
    "            dist_vec = np.linalg.norm(dist_vec, axis=2)\n",
    "            dist_vec = np.maximum(dist_vec, 5)\n",
    "\n",
    "            # find path loss // shadowing is not considered\n",
    "            pu_ch_gain_db = - PL_const - PL_alpha * np.log10(dist_vec)\n",
    "            pu_ch_gain = 10 ** (pu_ch_gain_db / 10)\n",
    "\n",
    "            multi_fading = 0.5 * np.random.randn(Num_D2D + 1, Num_D2D + 1) ** 2 + 0.5 * np.random.randn(Num_D2D + 1,\n",
    "                                                                                                        Num_D2D + 1) ** 2\n",
    "            final_ch = np.maximum(pu_ch_gain * multi_fading, np.exp(-30))\n",
    "            ch_w_temp_band.append(np.transpose(final_ch))\n",
    "\n",
    "        ch_w_fading.append(ch_w_temp_band)\n",
    "    return ch_w_fading\n",
    "\n",
    "\n",
    "'''\n",
    "cal_SINR_one_sample_one_channel: 生成每个样本的每个信道的SINR值\n",
    "'''\n",
    "def cal_SINR_one_sample_one_channel(channel, tx_power, noise):\n",
    "    ## Note that we transpose the channel to\n",
    "    diag_ch = np.diag(channel)\n",
    "    inter_ch = channel-np.diag(diag_ch)\n",
    "    tot_ch = np.multiply(channel, np.expand_dims(tx_power, -1))\n",
    "    int_ch = np.multiply(inter_ch, np.expand_dims(tx_power, -1))\n",
    "    sig_ch = np.sum(tot_ch-int_ch, axis=1)\n",
    "    int_ch = np.sum(int_ch, axis=1)\n",
    "\n",
    "    SINR_val = np.divide(sig_ch, int_ch+noise)\n",
    "    cap_val = np.log(1.0+SINR_val)\n",
    "    return cap_val\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "temp_power_function和all_possible_tx_power：生成所有可能的发射功率\n",
    "'''\n",
    "def temp_power_function(granuity, num_channel):\n",
    "    returned_array = []\n",
    "    total_iteration = int(granuity * num_channel + 1)\n",
    "    returned_array.append(np.zeros((num_channel,)))\n",
    "    for i in range(total_iteration - 1):\n",
    "        temp_iter = np.zeros((num_channel,))\n",
    "        selected_chanel = i // granuity\n",
    "        selected_power_val = i % granuity\n",
    "        temp_iter[selected_chanel] = (selected_power_val + 1) / granuity\n",
    "        returned_array.append(temp_iter)\n",
    "    return np.array(returned_array)\n",
    "\n",
    "\n",
    "def all_possible_tx_power(num_channel, num_user, granuty):\n",
    "    possible_power_for_one_user = np.arange(num_channel * granuty + 1)\n",
    "    a = np.arange(num_channel * granuty + 1) + 1\n",
    "    all_possible_int = np.array(list(product(a, repeat=num_user))) - 1\n",
    "    return temp_power_function(granuty, num_channel)[all_possible_int]\n",
    "\n",
    "\n",
    "'''\n",
    "optimal_power：找到最佳的发射功率\n",
    "'''\n",
    "def optimal_power(channel, tx_max, granuty, noise, DUE_thr, CUE_thr, tx_power_set):\n",
    "    num_channel = channel.shape[1]\n",
    "    ## Note that the num_user i\n",
    "    num_D2D_user = channel.shape[2] - 1\n",
    "    num_samples = channel.shape[0]\n",
    "    tot_cap = 0\n",
    "    tot_cap_CUE = 0\n",
    "    power_mat = []\n",
    "\n",
    "    # tot_success_prob counts the number of successful samples\n",
    "    tot_success_prob = 0\n",
    "\n",
    "    tx_power = tx_power_set\n",
    "    print(tx_power.shape)\n",
    "\n",
    "    tx_power = tx_max * np.hstack((tx_power, np.ones((tx_power.shape[0], 1, num_channel))))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        cur_cap = 0\n",
    "        DUE_mask = 1\n",
    "        CUE_mask = 1\n",
    "\n",
    "        for j in range(num_channel):\n",
    "            cur_ch = channel[i][j]\n",
    "            cur_ch_cap = cal_SINR_one_sample_one_channel(cur_ch, tx_power[:, :, j], noise)\n",
    "            cur_cap = cur_cap + cur_ch_cap\n",
    "            CUE_mask = CUE_mask * (cur_ch_cap[:, num_D2D_user] > CUE_thr)\n",
    "\n",
    "        for j in range(num_D2D_user):\n",
    "            DUE_mask = DUE_mask * (cur_cap[:, j] > DUE_thr)\n",
    "        D2D_sum = np.sum(cur_cap[:, :-1], axis=1)\n",
    "        CUE_sum = np.sum(cur_cap[:, -1:], axis=1)\n",
    "\n",
    "        D2D_sum = D2D_sum * CUE_mask * DUE_mask\n",
    "\n",
    "        max_cap = np.max(D2D_sum)\n",
    "        max_arg = np.argmax(D2D_sum)\n",
    "        max_cap_CUE = CUE_sum[max_arg]\n",
    "\n",
    "        if max_cap != 0:\n",
    "            tot_success_prob += 1\n",
    "            power_mat.append(tx_power[max_arg][:-1])\n",
    "        else:\n",
    "            power_mat.append(np.zeros(tx_power[0].shape)[:-1])\n",
    "\n",
    "\n",
    "        tot_cap = tot_cap + max_cap\n",
    "        tot_cap_CUE = tot_cap_CUE + max_cap_CUE\n",
    "\n",
    "    return tot_cap / num_samples / num_D2D_user, tot_cap_CUE / num_samples / num_channel, np.array(power_mat)\n",
    "\n",
    "\n",
    "'''\n",
    "optimal_power_check_valid：检测生成的信道是否满足条件\n",
    "'''\n",
    "def optimal_power_check_valid(channel, tx_max, granuty, noise, DUE_thr, CUE_thr):\n",
    "    num_channel = channel.shape[1]\n",
    "    ## Note that the num_user i\n",
    "    num_D2D_user = channel.shape[2] - 1\n",
    "\n",
    "    ## Feasible\n",
    "    feasible_channel_mat = []\n",
    "    infeasible_channel_mat = []\n",
    "\n",
    "    # tot_success_prob counts the number of successful samples\n",
    "    tx_power = np.zeros((1, num_D2D_user, num_channel))\n",
    "    tx_power = tx_max * np.hstack((tx_power, np.ones((tx_power.shape[0], 1, num_channel))))\n",
    "\n",
    "    for i in range(channel.shape[0]):\n",
    "        cur_cap = 0\n",
    "        CUE_mask = 1\n",
    "\n",
    "        for j in range(num_channel):\n",
    "            cur_ch = channel[i][j]\n",
    "            cur_ch_cap = cal_SINR_one_sample_one_channel(cur_ch, tx_power[:, :, j], noise)\n",
    "            cur_cap = cur_cap + cur_ch_cap\n",
    "            CUE_mask = CUE_mask * (cur_ch_cap[:, num_D2D_user] > CUE_thr)\n",
    "\n",
    "        if CUE_mask != 0:\n",
    "            feasible_channel_mat.append(channel[i])\n",
    "        else:\n",
    "            infeasible_channel_mat.append(channel[i])\n",
    "\n",
    "    return np.array(feasible_channel_mat), np.array(infeasible_channel_mat)\n",
    "\n",
    "\n",
    "'''\n",
    "convert_optimal_power：最佳功率，后续的graph label\n",
    "'''\n",
    "def convert_optimal_power(tx_power_mat, tx_max, Num_power_level, Num_channel):\n",
    "    num_samples = tx_power_mat.shape[0]\n",
    "    num_user = tx_power_mat.shape[1]\n",
    "    resource_alloc = []\n",
    "    for i in range(num_samples):\n",
    "        resource_alloc_inner = []\n",
    "        for j in range(num_user):\n",
    "            channel_select = np.argmax(tx_power_mat[i, j])\n",
    "            power_select = np.round(tx_power_mat[i, j, channel_select] / tx_max * (Num_power_level - 1))\n",
    "            resource_alloc_mat = np.zeros((Num_power_level + Num_channel,))\n",
    "            resource_alloc_mat[int(power_select)] = 1\n",
    "            resource_alloc_mat[int(Num_power_level + channel_select)] = 1\n",
    "            resource_alloc_inner.append(resource_alloc_mat)\n",
    "        resource_alloc.append(np.array(resource_alloc_inner))\n",
    "    return np.array(resource_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b227a2",
   "metadata": {},
   "source": [
    "### GCN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2431b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "hidden_dim = 400 ## 隐藏层神经元个数\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes, n_classes_2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_dim, hidden_dim).to('cuda:0')\n",
    "        self.conv2 = dglnn.GraphConv(hidden_dim, hidden_dim).to('cuda:0')\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim).to('cuda:0')\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes).to('cuda:0')\n",
    "        self.drop = nn.Dropout(0.02).to('cuda:0')\n",
    "        self.classify_2 = nn.Linear(hidden_dim, n_classes_2).to('cuda:0')\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # 应用图卷积和激活函数\n",
    "        h_1_res = self.conv1(g, h) ##Dense\n",
    "        \n",
    "        h_1 = F.relu(self.norm(h_1_res))\n",
    "        h_1 = self.conv2(g, h_1) \n",
    "        \n",
    "        for i in range(6):\n",
    "            h_1 = self.norm(h_1)\n",
    "            h_1 += h_1_res ## 残差\n",
    "            h_1 = F.relu(h_1)\n",
    "            h_1 = self.drop(h_1)\n",
    "            h_1 = self.conv2(g, h_1)\n",
    "        \n",
    "        h_1_out = self.norm(h_1)\n",
    "        h_1_out += h_1_res ## 残差\n",
    "        h_1_out = F.relu(h_1_out)\n",
    "        ############################\n",
    "        \n",
    "        # 应用图卷积和激活函数\n",
    "        h_2_res = self.conv1(g, h) ##Dense\n",
    "        h_2 = F.relu(self.norm(h_2_res))\n",
    "        h_2 = self.conv2(g, h_2) \n",
    "        \n",
    "        for i in range(6):\n",
    "            h_2 = self.norm(h_2)\n",
    "            h_2 += h_2_res ## 残差\n",
    "            h_2 = F.relu(h_2)\n",
    "            h_2 = self.drop(h_2)\n",
    "            h_2 = self.conv2(g, h_2)\n",
    "        \n",
    "        h_2_out = self.norm(h_2)\n",
    "        h_2_out += h_2_res ## 残差\n",
    "        h_2_out = F.relu(h_2_out)\n",
    "        ############################\n",
    "        \n",
    "        \n",
    "        \n",
    "        with g.local_scope():\n",
    "            g.ndata['h_1'] = h_1_out\n",
    "            # 使用平均读出计算图表示,##一个信息汇聚的过程\n",
    "            hg = dgl.max_nodes(g, 'h_1')\n",
    "            final_out = self.classify(hg)\n",
    "\n",
    "            g.ndata['h_2'] = h_2_out\n",
    "            # 使用平均读出计算图表示,##一个信息汇聚的过程\n",
    "            hg_2 = dgl.max_nodes(g, 'h_2')\n",
    "            final_out_2 = self.classify_2(hg_2)\n",
    "\n",
    "            \n",
    "            final_out = torch.reshape(final_out,(len(final_out), 3, 8))\n",
    "            final_out_2 = torch.reshape(final_out_2,(len(final_out_2), 3, 3))\n",
    "            \n",
    "            h_final = torch.cat([final_out,final_out_2],dim=2) ##聚合两个模型的输出3*11\n",
    "            \n",
    "            return h_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce3247",
   "metadata": {},
   "source": [
    "### 监督学习训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 正式开始执行\n",
    "Num_user = 3\n",
    "Num_channel = 3 \n",
    "Num_power_level = 8\n",
    "\n",
    "\n",
    "BW = 1e7 ## 带宽\n",
    "noise = BW*10**-17.4 ## 噪声功率\n",
    "num_samples_init = int(1e6) ## 非监督学习样本数\n",
    "num_samples_test = int(1e5) ## 测试集样本数\n",
    "num_init = int(50000) ## 监督学习样本数\n",
    "Size_area = 100  ## 区域面积100*100\n",
    "D2D_dist = 30  ## D2D的距离\n",
    "batch_size_set = 1024\n",
    "\n",
    "\n",
    "DUE_thr = -0.5\n",
    "tx_max = 10**2.3\n",
    "channel_error = 0.0\n",
    "\n",
    "epoch_num_init = 7\n",
    "epoch_num = 100\n",
    "\n",
    "CUE_thr = 0.6931 * 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4760b3",
   "metadata": {},
   "source": [
    "#### 1：生成训练集和标签(初始化训练是监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating all channel\n",
    "data_train_full = np.array(ch_gen(Size_area, D2D_dist, Num_user, Num_channel, num_samples_init), dtype=np.float32)\n",
    "\n",
    "## check generated channel \n",
    "data_train, data_train_infeasible = optimal_power_check_valid(data_train_full, tx_max, Num_power_level, noise, DUE_thr, CUE_thr)\n",
    "\n",
    "print(\"Number of feasible sets:  \", data_train.shape)\n",
    "print(\"Number of INfeasible sets:  \", data_train_infeasible.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## Generate the train channel set\n",
    "data_train = data_train[:batch_size_set * (data_train.shape[0] // batch_size_set)]\n",
    "# ## Recalculate the number of feasible solutions\n",
    "num_samples = data_train.shape[0]\n",
    "labels = np.zeros(num_samples, )\n",
    "log_data = np.log(data_train)\n",
    "log_data_mean = np.mean(log_data)\n",
    "log_data_std = np.std(log_data)\n",
    "log_data = (log_data - log_data_mean) / log_data_std\n",
    "\n",
    "print(log_data.shape)\n",
    "\n",
    "################################################################################\n",
    "## Generate train lable\n",
    "################################################################################\n",
    "tx_power_set = all_possible_tx_power(Num_channel, Num_user, Num_power_level - 1)\n",
    "print(\"Preprecess of data is finished\")\n",
    "time_cur = time.time()\n",
    "OPT_DUE_train, OPT_CUE_train, opt_power = optimal_power(data_train[:num_init], tx_max, Num_power_level, noise, DUE_thr,\n",
    "                                                        CUE_thr, tx_power_set)\n",
    "\n",
    "\n",
    "print(\"cur_time 1 : \", time.time() - time_cur)\n",
    "time_cur = time.time()\n",
    "opt_power_label = convert_optimal_power(opt_power, tx_max, Num_power_level, Num_channel)\n",
    "print(\"cur_time 2: \", time.time() - time_cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4232e9",
   "metadata": {},
   "source": [
    "#### 2：构建子图(初始化训练是监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### step1:三个信道的干涉相拼接\n",
    "user_num = 3\n",
    "channel_num = 3\n",
    "\n",
    "src_index = []\n",
    "dst_index = []\n",
    "for i in range(user_num+1):\n",
    "    for j in range(user_num+1):\n",
    "        src_index.append(i)\n",
    "        dst_index.append(j)\n",
    "\n",
    "print(src_index)\n",
    "print(dst_index)\n",
    "\n",
    "#### step2:构图\n",
    "class Power_Allocation_Dataset_Init(DGLDataset):\n",
    "    def __init__(self, index_g):\n",
    "        self.index_g = index_g\n",
    "        super().__init__(name='Power_Allocation')\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        self.graphs, self.labels = [],[]\n",
    "        \n",
    "        for index_ in self.index_g:\n",
    "            \n",
    "            feature = log_data[index_]\n",
    "            feat = log_data[index_].reshape((3 * 4 * 4))\n",
    "            \n",
    "            all_feat = []\n",
    "            for _ in range(user_num+1):\n",
    "                all_feat.append(feat)\n",
    "            all_feat = np.array(all_feat)\n",
    "            # edges_data = edges_data_all[index_]\n",
    "            \n",
    "            weight_int = []\n",
    "            ## 三个信道的干涉相加\n",
    "            for i in range(len(src_index)):\n",
    "                tmp_weight = 0\n",
    "                for c in range(channel_num):\n",
    "                    tmp_weight += feature[c][dst_index[i]][src_index[i]]\n",
    "                weight_int.append(tmp_weight)\n",
    "            \n",
    "            \n",
    "            node_features = torch.from_numpy(all_feat).to(torch.float32)\n",
    "            graph_labels = torch.from_numpy(opt_power_label[index_]).to(torch.int64)\n",
    "            \n",
    "            ## 将图中边的信息转为Tensor类型\n",
    "            edge_features = torch.from_numpy(np.array(weight_int))\n",
    "            edges_src = torch.from_numpy(np.array(src_index)).to(torch.int32)\n",
    "            edges_dst = torch.from_numpy(np.array(dst_index)).to(torch.int32)\n",
    "\n",
    "            self.graph = dgl.graph((edges_src, edges_dst), num_nodes=user_num+1) \n",
    "            self.graph.ndata['attr'] = node_features\n",
    "#             self.graph.ndata['label'] = graph_labels ## 无节点label\n",
    "            self.graph.edata['weight'] = edge_features\n",
    "\n",
    "    \n",
    "            ## save sub-figure and graph_label\n",
    "            self.graphs.append(self.graph.to('cuda:0'))\n",
    "            self.labels.append(graph_labels.to('cuda:0'))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "\n",
    "dataset_train = Power_Allocation_Dataset_Init(np.arange(num_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### step3：batch_size\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataloader_train_init = GraphDataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size, ##决定分为几个batch进行训练\n",
    "    drop_last=False,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf2c39",
   "metadata": {},
   "source": [
    "#### 3：训练模型(初始化训练是监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练模型\n",
    "feature_dim = 48\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = Classifier(feature_dim, 3*8, 3*3)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-3) ##weight_decay=5e-4\n",
    "\n",
    "time_cur = time.time()\n",
    "\n",
    "val_acc_best = 0\n",
    "for epoch in tqdm(range(7)):\n",
    "    all_loss = 0\n",
    "    for batched_graph, labels in dataloader_train_init: ##fox\n",
    "#         print(batched_graph)\n",
    "        feats = batched_graph.ndata['attr']\n",
    "        logits_ = model(batched_graph, feats)\n",
    "#         print(logits)\n",
    "        logits = torch.transpose(logits_,1,2)\n",
    "        logits_1 = logits[:,:8,:]\n",
    "        logits_2 = logits[:,8:,:]\n",
    "        \n",
    "        logits_1 = F.softmax(logits_1, dim=1)\n",
    "        logits_2 = F.softmax(logits_2, dim=1)\n",
    "        \n",
    "        logits_binary = torch.cat([torch.transpose(logits_1,1,2),torch.transpose(logits_2,1,2)],dim=2)\n",
    "        \n",
    "        loss_1 = F.cross_entropy(logits_1, labels[:,:,:8].argmax(2))\n",
    "        loss_2 = F.cross_entropy(logits_2, labels[:,:,8:].argmax(2))\n",
    "        loss_3 = -torch.mean(torch.mean(torch.pow(logits_binary-0.5, 4), axis=2))\n",
    "    \n",
    "        loss = loss_1+loss_2+loss_3\n",
    "        all_loss += loss.item()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"loss: \", all_loss/len(dataloader_train_init))\n",
    "        print(\"Up_to_now_consume_time: \", time.time() - time_cur)        \n",
    "        \n",
    "torch.save(model, 'init_network.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7eb5ec",
   "metadata": {},
   "source": [
    "### 非监督学习训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 参数设置\n",
    "\n",
    "print(\"\")\n",
    "print(\"Outer_loop: %d \" % outer_loop)\n",
    "print(\"CUE thr: %0.2f \" % (CUE_thr / 0.6931))\n",
    "\n",
    "lambda_mat = np.ones((4, 1))\n",
    "coeff_factor = 1.0\n",
    "lambda_mat[0] = 1.0 * coeff_factor\n",
    "lambda_mat[1] = 70.0 * coeff_factor\n",
    "lambda_mat[2] = 15.0 * coeff_factor\n",
    "lambda_mat[3] = 0.0\n",
    "\n",
    "lambda_mat = torch.from_numpy(lambda_mat).to(torch.float32).to('cuda:0')\n",
    "\n",
    "print(\"lambda\")\n",
    "print(lambda_mat)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb6873",
   "metadata": {},
   "source": [
    "#### 1：构建子图(非监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 三个信道的干涉相拼接\n",
    "user_num = 3\n",
    "channel_num = 3\n",
    "\n",
    "src_index = []\n",
    "dst_index = []\n",
    "for i in range(user_num+1):\n",
    "    for j in range(user_num+1):\n",
    "        src_index.append(i)\n",
    "        dst_index.append(j)\n",
    "\n",
    "print(src_index)\n",
    "print(dst_index)\n",
    "\n",
    "#### 构图\n",
    "class Power_Allocation_Dataset(DGLDataset):\n",
    "    def __init__(self, index_g, log_data):\n",
    "        self.index_g = index_g\n",
    "        self.log_data = log_data\n",
    "        super().__init__(name='Power_Allocation')\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "#         self.graphs, self.labels = [],[]\n",
    "        self.graphs,self.data_org = [],[]\n",
    "        for index_ in self.index_g:\n",
    "            \n",
    "            feature = self.log_data[index_]\n",
    "            feat = self.log_data[index_].reshape((3 * 4 * 4))\n",
    "            \n",
    "            all_feat = []\n",
    "            for _ in range(user_num+1):\n",
    "                all_feat.append(feat)\n",
    "            all_feat = np.array(all_feat)\n",
    "            # edges_data = edges_data_all[index_]\n",
    "            \n",
    "            weight_int = []\n",
    "            ## 三个信道的干涉相加\n",
    "            for i in range(len(src_index)):\n",
    "                tmp_weight = 0\n",
    "                for c in range(channel_num):\n",
    "                    tmp_weight += feature[c][dst_index[i]][src_index[i]]\n",
    "                weight_int.append(tmp_weight)\n",
    "            \n",
    "#             print(all_feat)\n",
    "            node_features = torch.from_numpy(all_feat).to(torch.float32)\n",
    "#             graph_labels = torch.from_numpy(opt_power_label[index_]).to(torch.int64)\n",
    "            \n",
    "            ## 将图中边的信息转为Tensor类型\n",
    "            edge_features = torch.from_numpy(np.array(weight_int))\n",
    "            edges_src = torch.from_numpy(np.array(src_index)).to(torch.int32)\n",
    "            edges_dst = torch.from_numpy(np.array(dst_index)).to(torch.int32)\n",
    "\n",
    "            self.graph = dgl.graph((edges_src, edges_dst), num_nodes=user_num+1)\n",
    "            self.graph.ndata['attr'] = node_features\n",
    "#             self.graph.ndata['label'] = graph_labels ## 无节点label\n",
    "            self.graph.edata['weight'] = edge_features\n",
    "\n",
    "            self.feature = torch.from_numpy(feature).to(torch.float32)\n",
    "            ## save sub-figure and graph_label\n",
    "            self.graphs.append(self.graph.to('cuda:0'))\n",
    "            self.data_org.append(self.feature.to('cuda:0'))\n",
    "#             self.labels.append(graph_labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i],self.data_org[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "dataset_train_all = Power_Allocation_Dataset(np.arange(len(log_data)), log_data)\n",
    "print(\"*************************完整的训练集构图完毕*************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch_size化\n",
    "batch_size_set = 1024\n",
    "dataloader_train_true_batch = GraphDataLoader(\n",
    "    dataset_train_all,\n",
    "    batch_size=batch_size_set, ##构成一个完整的sample后续使用\n",
    "    drop_last=False, ## 该参数未知\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca57c1",
   "metadata": {},
   "source": [
    "#### 2：训练模型(非监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647af1d2-6f3f-4124-bfb1-7b56889d3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cal_RATE_tf和cal_LOSS_Total_tf：模型非监督学习过程的loss\n",
    "'''\n",
    "def cal_RATE_tf(channel, tx_power, tx_max, noise, num_samples, log_data_mean, log_data_std):\n",
    "    chan_num = channel.shape[1]\n",
    "    user_num = channel.shape[2]\n",
    "    cap_val = torch.from_numpy(np.array(0.0)).to(torch.float32).to('cuda:0')\n",
    "    CUE_cap = []\n",
    "    \n",
    "    channel_rev = tf.exp(channel * log_data_std + log_data_mean).to('cuda:0')\n",
    "\n",
    "    for i in range(chan_num):\n",
    "        tx_power_w_CUE = tf.cat([tx_power[:, :, i], tx_max * tf.ones((num_samples, 1)).to('cuda:0')], axis=1)\n",
    "        tot_ch = tf.multiply(channel_rev[:, i], tx_power_w_CUE.unsqueeze(-1))\n",
    "        sig_ch = []\n",
    "        for ch in tot_ch:\n",
    "            sig_ch.append(tf.diag(ch,0).cpu().detach().numpy())\n",
    "        sig_ch = torch.from_numpy(np.array(sig_ch)).to(torch.float32).to('cuda:0')\n",
    "\n",
    "        ###生成一个除了对角线均为0的矩阵 #tf.linalg.diag(sig_ch) ## 由于torch没有这个函数，因此自己构造\n",
    "        diag_0 = tot_ch\n",
    "        for i in range(len(diag_0)):\n",
    "            for j in range(len(diag_0[i])):\n",
    "                for k in range(len(diag_0[j])):\n",
    "                    if j != k:\n",
    "                        diag_0[i][j][k] = 0\n",
    "#         print(diag_0)\n",
    "        inter_ch = tot_ch - diag_0\n",
    "        \n",
    "        inter_ch = tf.sum(inter_ch, dim=1)\n",
    "        SINR_val = tf.div(sig_ch, inter_ch + noise)\n",
    "        cap_val = cap_val + tf.log(torch.from_numpy(np.array(1.0)).to(torch.float32).to('cuda:0') + SINR_val)\n",
    "        CUE_cap.append(tf.log(torch.from_numpy(np.array(1.0)).to(torch.float32).to('cuda:0') + SINR_val)[:, -1].cpu().detach().numpy())\n",
    "        \n",
    "    CUE_cap = torch.from_numpy(np.array(CUE_cap)).to(torch.float32).to('cuda:0')\n",
    "    cap_val_D2D = cap_val[:, :user_num - 1]\n",
    "\n",
    "    return cap_val_D2D, tf.transpose(CUE_cap, 0, 1)\n",
    "\n",
    "\n",
    "def cal_LOSS_Total_tf(channel, tf_output, noise, DUE_thr, CUE_thr, tx_max, num_samples, log_data_mean, log_data_std,\n",
    "                      lambda_mat):\n",
    "    ## Output of DNN should be divided proerly.\n",
    "    ## [power alloc, resource alloc on channel]\n",
    "    chan_num = int(channel.shape[1])\n",
    "    user_num = int(channel.shape[2] - 1)\n",
    "    power_granu = int(Num_power_level)\n",
    "    power_mat = torch.from_numpy(np.arange(power_granu) / (power_granu - 1)).to(torch.float32)\n",
    "    tx_pow_level = tf_output[:, :, :power_granu] * power_mat.to('cuda:0')\n",
    "#     print(\"tx_pow_level: \", tx_pow_level)\n",
    "    tx_pow_level_tot = tf.sum(tx_pow_level, dim=2) * tx_max\n",
    "    tx_pow_level_tot = tf.reshape(tx_pow_level_tot, [-1, user_num, 1])\n",
    "    tx_pow_chan = tf.multiply(tf_output[:, :, power_granu:power_granu+chan_num], tx_pow_level_tot)\n",
    "\n",
    "    D2D_rate, CUE_rate = cal_RATE_tf(channel, tx_pow_chan, tx_max, noise, num_samples, log_data_mean, log_data_std)\n",
    "\n",
    "    CUE_vio = tf.nn.functional.relu(CUE_thr - CUE_rate) / (CUE_thr + 1e-10)\n",
    "    integer_vio_1 = -tf.sum(tf.abs(tf_output[:, :, :power_granu+chan_num] - 0.5))\n",
    "    integer_vio_2 = -tf.sum(tf.abs(tf_output[:, :, power_granu + chan_num:] - 0.5))\n",
    "    CUE_vio_sum = tf.sum(CUE_vio)\n",
    "\n",
    "    Loss = - tf.sum(lambda_mat[0] * D2D_rate)  + lambda_mat[1] * CUE_vio_sum + lambda_mat[2] * integer_vio_1 + lambda_mat[3] * integer_vio_2\n",
    "    return Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_cur = 1e-6\n",
    "\n",
    "print(\"\")\n",
    "print(\"learning_rate : \", learning_rate_cur)\n",
    "print(\"lambda : \", lambda_mat)\n",
    "\n",
    "model = torch.load('init_network.pth')\n",
    "\n",
    "\n",
    "print(\"model loading finished\")\n",
    "print(\"\")\n",
    "\n",
    "opt_2 = torch.optim.Adam(model.parameters(),lr=learning_rate_cur) ##, weight_decay=5e-4\n",
    "\n",
    "### 开始训练\n",
    "for i in range(10):\n",
    "    if not os.path.exists('./' + str(i) + '-saved_model/'): #判断所在目录下是否有该文件名的文件夹\n",
    "        os.mkdir('./' + str(i) + '-saved_model/') #创建多级目录用mkdirs，单击目录mkdir\n",
    "        \n",
    "    for epoch in range(20):\n",
    "        all_loss = 0\n",
    "        for batched_graph, data_org in tqdm(dataloader_train_true_batch): ##fox\n",
    "#             print(batched_graph)\n",
    "            feats = batched_graph.ndata['attr']\n",
    "            logits_ = model(batched_graph, feats)\n",
    "            ## 分别做softmax, 并聚合成3*11\n",
    "            logits_1 = F.softmax(logits_[:,:,:8], dim=2)\n",
    "            logits_2 = F.softmax(logits_[:,:,8:], dim=2)\n",
    "            logits = torch.cat([logits_1,logits_2],dim=2)\n",
    "            \n",
    "#             print(\"data_org.shape: \", data_org.shape)\n",
    "            \n",
    "            ## 这里的log_data一定是batch_size大小的，不然无法与ligits匹配\n",
    "            loss_2 = cal_LOSS_Total_tf(data_org, logits, noise, DUE_thr, CUE_thr, tx_max, batch_size_set, log_data_mean, \n",
    "                                     log_data_std, lambda_mat)\n",
    "\n",
    "            all_loss += loss_2.item()\n",
    "                \n",
    "            opt_2.zero_grad()\n",
    "            loss_2.backward()\n",
    "            opt_2.step()\n",
    "\n",
    "        print(str(epoch) + \"---------------------------------------------> True_loss: \", all_loss/len(dataloader_train_true_batch))    \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(model, './' + str(i) + '-saved_model/' + str(epoch) + '-network.pth')  \n",
    "    \n",
    "       \n",
    "\n",
    "    \n",
    "    print(\"\")\n",
    "    print(str(i) + \"-> full model training is finished\")\n",
    "    print(\"\")\n",
    "    torch.save(model, 'Ture-' + str(i) + '-network.pth') ##保存这一次随机生成的数据集训练的参数结果\n",
    "    model = torch.load('Ture-' + str(i) + '-network.pth')\n",
    "    \n",
    "    print(\"\")\n",
    "    print(str(i) + \"-> generating new data for training\")\n",
    "    print(\"\")\n",
    "    data_train_full = np.array(ch_gen(Size_area, D2D_dist, Num_user, Num_channel, num_samples_init),\n",
    "                               dtype=np.float32)\n",
    "    \n",
    "    data_train, data_train_infeasible = optimal_power_check_valid(data_train_full, tx_max, Num_power_level,\n",
    "                                                                  noise,\n",
    "                                                                  DUE_thr, CUE_thr)\n",
    "\n",
    "    data_train = data_train[:batch_size_set * (data_train.shape[0] // batch_size_set)]\n",
    "\n",
    "    ## Recalculate the number of feasible solutions\n",
    "    num_samples = data_train.shape[0]\n",
    "#     labels = np.zeros(num_samples, )\n",
    "    log_data = np.log(data_train)\n",
    "    log_data = (log_data - log_data_mean) / log_data_std\n",
    "    \n",
    "    print(\"\")\n",
    "    print(str(i) + \"-> constructing new graph for training\")\n",
    "    print(\"\")\n",
    "    \n",
    "    ## 重新构图\n",
    "    dataset_train_all = Power_Allocation_Dataset(np.arange(len(log_data)), log_data)\n",
    "    \n",
    "    batch_size_set = 1024\n",
    "    \n",
    "    dataloader_train_true_batch = GraphDataLoader(\n",
    "    dataset_train_all,\n",
    "    batch_size=batch_size_set, ##构成一个完整的sample后续使用\n",
    "    drop_last=False, ## 该参数未知\n",
    "    shuffle=False)\n",
    "    \n",
    "    \n",
    "    print(\"%d-th iteration is finished  \" % i)\n",
    "    print(\"CUE-Thr   ---   %0.2f\" % (CUE_thr / 0.6931))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
